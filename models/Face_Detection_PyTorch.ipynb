{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\repos\\Models-Inferencing-On-Client-Sandbox\\sandboxenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"hustvl/yolos-tiny\"\n",
    "teacher_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "teacher_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "train_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/train\"\n",
    "test_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/test\"\n",
    "validation_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/validation\"\n",
    "\n",
    "is_data_local: bool = os.path.exists(train_dir) and os.path.exists(test_dir) and os.path.exists(validation_dir)\n",
    "is_using_subset: bool = True\n",
    "\n",
    "if is_data_local:\n",
    "    train_dataset = load_from_disk(train_dir)\n",
    "    test_dataset = load_from_disk(test_dir)\n",
    "    validation_dataset = load_from_disk(validation_dir)\n",
    "else:\n",
    "    train_dataset = load_dataset(\"wider_face\", split=\"train[:5%]\")\n",
    "    test_dataset = load_dataset(\"wider_face\", split=\"test[:5%]\")\n",
    "    validation_dataset = load_dataset(\"wider_face\", split=\"validation[:5%]\")\n",
    "    train_dataset.save_to_disk(train_dir)\n",
    "    test_dataset.save_to_disk(test_dir)\n",
    "    validation_dataset.save_to_disk(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 644\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 805\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 161\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': validation_dataset,\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_coco_fields(example, idx):\n",
    "    bboxes = example['faces']['bbox']\n",
    "    areas = [bbox[2] * bbox[3] for bbox in bboxes]\n",
    "    \n",
    "    example['image_id'] = idx\n",
    "    example['faces']['area'] = areas\n",
    "    example['faces']['category'] = [0 for _ in example[\"faces\"][\"expression\"]]\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(add_coco_fields, with_indices=True)\n",
    "    \n",
    "categories = [\"face\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1385>,\n",
       " 'faces': {'area': [18178.0],\n",
       "  'bbox': [[449.0, 330.0, 122.0, 149.0]],\n",
       "  'blur': [0],\n",
       "  'category': [0],\n",
       "  'expression': [0],\n",
       "  'illumination': [0],\n",
       "  'invalid': [False],\n",
       "  'occlusion': [0],\n",
       "  'pose': [0]},\n",
       " 'image_id': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(480, 480),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 1 if len(bbox)>1 else 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_aug_ann(examples):\n",
    "    processed_images = []  # This will hold the processed 'pixel_values'\n",
    "    labels = []  # This will hold the 'labels' dicts for each image\n",
    "    \n",
    "    for idx, (image, faces) in enumerate(zip(examples[\"image\"], examples[\"faces\"])):\n",
    "        # Convert PIL image to numpy array and ensure RGB order\n",
    "        image_np = np.array(image.convert(\"RGB\"))\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed = transform(image=image_np, bboxes=faces[\"bbox\"], category=faces[\"category\"])\n",
    "        \n",
    "        # Process transformed image with teacher_processor to get pixel_values\n",
    "        pixel_values = teacher_processor(images=transformed[\"image\"], return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Construct labels dict\n",
    "        image_labels = {\n",
    "            'size': torch.tensor([480, 480], dtype=torch.long),\n",
    "            'image_id': torch.tensor([examples[\"image_id\"][idx]], dtype=torch.long),\n",
    "            'class_labels': torch.tensor(faces[\"category\"], dtype=torch.long),\n",
    "            'boxes': torch.tensor(transformed[\"bboxes\"]).float(),\n",
    "            'area': torch.tensor(faces[\"area\"]),\n",
    "            'iscrowd': torch.tensor([1 if len(faces[\"bbox\"]) > 1 else 0]),\n",
    "            'orig_size': torch.tensor([image.size[1], image.size[0]])  # Height, Width\n",
    "        }\n",
    "        \n",
    "        # Append the processed pixel_values and labels\n",
    "        pixel_values = pixel_values.squeeze(0) # Remove extra dim\n",
    "        processed_images.append(pixel_values)\n",
    "        labels.append(image_labels)\n",
    "    \n",
    "    # Convert lists to tensors for batching\n",
    "    pixel_values_batch = torch.stack(processed_images)\n",
    "    # Labels need to be a list of dicts, each dict corresponding to an image\n",
    "    \n",
    "    return {\"pixel_values\": pixel_values_batch, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.7933, -1.0733, -1.7069,  ..., -1.9467, -1.9467, -1.9467],\n",
       "          [ 0.8104, -1.3473, -1.9467,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.8104, -1.3815, -1.9809,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [ 0.8276, -1.7240, -2.1179,  ..., -1.9467, -1.9295, -1.9467],\n",
       "          [ 0.8276, -1.6042, -2.1008,  ..., -1.9980, -1.9980, -2.0665],\n",
       "          [ 0.8276, -1.6042, -2.0837,  ..., -2.1179, -2.0152, -1.8782]],\n",
       " \n",
       "         [[ 0.9230, -0.9328, -1.5455,  ..., -1.6506, -1.6506, -1.6506],\n",
       "          [ 0.9230, -1.2654, -1.8431,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.9055, -1.2829, -1.8782,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [ 0.9755, -1.6331, -2.0357,  ..., -1.7381, -1.7381, -1.7556],\n",
       "          [ 0.9755, -1.5105, -1.9832,  ..., -1.8081, -1.8081, -1.8782],\n",
       "          [ 0.9580, -1.4930, -1.9832,  ..., -1.9657, -1.8431, -1.7031]],\n",
       " \n",
       "         [[ 1.0539, -0.5844, -1.0201,  ..., -0.9330, -0.9330, -0.9330],\n",
       "          [ 1.1759, -0.6541, -1.0376,  ..., -0.9678, -0.9678, -0.9678],\n",
       "          [ 1.1934, -0.6715, -1.0376,  ..., -0.8110, -0.8110, -0.8110],\n",
       "          ...,\n",
       "          [ 1.0888, -1.5953, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 1.1062, -1.4733, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 1.0714, -1.4559, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " 'labels': {'size': tensor([480, 480]),\n",
       "  'image_id': tensor([0]),\n",
       "  'class_labels': tensor([0]),\n",
       "  'boxes': tensor([[212.3438, 114.3682,  57.1875,  51.6390]]),\n",
       "  'area': tensor([18178.]),\n",
       "  'iscrowd': tensor([0]),\n",
       "  'orig_size': tensor([1385, 1024])}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = teacher_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuned-yolos-tiny\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\repos\\Models-Inferencing-On-Client-Sandbox\\sandboxenv\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  6%|▌         | 50/810 [00:15<03:43,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2063.6431, 'grad_norm': 42.59532165527344, 'learning_rate': 9.419753086419753e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 100/810 [00:30<03:30,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1899.5256, 'grad_norm': 39.595062255859375, 'learning_rate': 8.814814814814817e-06, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 150/810 [00:45<03:11,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1996.6937, 'grad_norm': 35.58168029785156, 'learning_rate': 8.197530864197532e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 200/810 [01:00<03:07,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2055.2484, 'grad_norm': 28.648643493652344, 'learning_rate': 7.580246913580247e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 250/810 [01:15<02:44,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1844.985, 'grad_norm': 18.545175552368164, 'learning_rate': 6.962962962962964e-06, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 300/810 [01:31<02:47,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1855.3216, 'grad_norm': 8.46521282196045, 'learning_rate': 6.345679012345679e-06, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 350/810 [01:46<02:18,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2023.9447, 'grad_norm': 6.406212329864502, 'learning_rate': 5.728395061728396e-06, 'epoch': 4.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 400/810 [02:01<01:59,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2005.1331, 'grad_norm': 6.289460182189941, 'learning_rate': 5.1111111111111115e-06, 'epoch': 4.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 450/810 [02:17<01:51,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2087.0244, 'grad_norm': 8.88055419921875, 'learning_rate': 4.493827160493827e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 500/810 [02:33<01:39,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1858.4398, 'grad_norm': 10.83547592163086, 'learning_rate': 3.876543209876544e-06, 'epoch': 6.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 550/810 [02:48<01:24,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1977.4927, 'grad_norm': 9.50704288482666, 'learning_rate': 3.25925925925926e-06, 'epoch': 6.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 600/810 [03:03<01:04,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1975.0391, 'grad_norm': 10.463424682617188, 'learning_rate': 2.6419753086419752e-06, 'epoch': 7.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 650/810 [03:19<00:44,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1999.7152, 'grad_norm': 12.629549980163574, 'learning_rate': 2.0246913580246915e-06, 'epoch': 8.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 700/810 [03:34<00:32,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2013.5748, 'grad_norm': 10.011380195617676, 'learning_rate': 1.4074074074074075e-06, 'epoch': 8.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 750/810 [03:49<00:17,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1886.3752, 'grad_norm': 10.846943855285645, 'learning_rate': 7.901234567901235e-07, 'epoch': 9.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 800/810 [04:04<00:03,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1989.5583, 'grad_norm': 11.371615409851074, 'learning_rate': 1.728395061728395e-07, 'epoch': 9.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [04:07<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 247.8369, 'train_samples_per_second': 25.985, 'train_steps_per_second': 3.268, 'train_loss': 1970.4979021990741, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=1970.4979021990741, metrics={'train_runtime': 247.8369, 'train_samples_per_second': 25.985, 'train_steps_per_second': 3.268, 'train_loss': 1970.4979021990741, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset[\"train\"], # type: ignore\n",
    "    tokenizer=teacher_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuned_yolostiny_model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_path = \"tuned_yolostiny_model\"\n",
    "\n",
    "teacher_model.save_pretrained(tuned_model_path)\n",
    "teacher_processor.save_pretrained(tuned_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandboxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
