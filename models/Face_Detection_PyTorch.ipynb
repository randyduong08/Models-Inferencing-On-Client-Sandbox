{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\repos\\Models-Inferencing-On-Client-Sandbox\\sandboxenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"hustvl/yolos-tiny\"\n",
    "teacher_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "teacher_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "train_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/train\"\n",
    "test_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/test\"\n",
    "validation_dir = \"C:/source/repos/Models-Inferencing-On-Client-Sandbox/datasets/image-face-dataset/validation\"\n",
    "\n",
    "is_data_local: bool = os.path.exists(train_dir) and os.path.exists(test_dir) and os.path.exists(validation_dir)\n",
    "is_using_subset: bool = True\n",
    "\n",
    "if is_data_local:\n",
    "    train_dataset = load_from_disk(train_dir)\n",
    "    test_dataset = load_from_disk(test_dir)\n",
    "    validation_dataset = load_from_disk(validation_dir)\n",
    "else:\n",
    "    train_dataset = load_dataset(\"wider_face\", split=\"train[:5%]\")\n",
    "    test_dataset = load_dataset(\"wider_face\", split=\"test[:5%]\")\n",
    "    validation_dataset = load_dataset(\"wider_face\", split=\"validation[:5%]\")\n",
    "    train_dataset.save_to_disk(train_dir)\n",
    "    test_dataset.save_to_disk(test_dir)\n",
    "    validation_dataset.save_to_disk(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 644\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 805\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'faces'],\n",
       "        num_rows: 161\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': validation_dataset,\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_coco_fields(example, idx):\n",
    "    bboxes = example['faces']['bbox']\n",
    "    areas = [bbox[2] * bbox[3] for bbox in bboxes]\n",
    "    \n",
    "    example['image_id'] = idx\n",
    "    example['faces']['area'] = areas\n",
    "    example['faces']['category'] = [0 for _ in example[\"faces\"][\"expression\"]]\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(add_coco_fields, with_indices=True)\n",
    "    \n",
    "categories = [\"face\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1385>,\n",
       " 'faces': {'area': [18178.0],\n",
       "  'bbox': [[449.0, 330.0, 122.0, 149.0]],\n",
       "  'blur': [0],\n",
       "  'category': [0],\n",
       "  'expression': [0],\n",
       "  'illumination': [0],\n",
       "  'invalid': [False],\n",
       "  'occlusion': [0],\n",
       "  'pose': [0]},\n",
       " 'image_id': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(480, 480),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 1 if len(bbox)>1 else 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, faces in zip(examples[\"image\"], examples[\"faces\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=faces[\"bbox\"], category=faces[\"category\"])\n",
    "\n",
    "        area.append(faces[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "    \n",
    "    print(targets)\n",
    "\n",
    "    return teacher_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].with_transform(transform_aug_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 0, 'annotations': [{'image_id': 0, 'category_id': 0, 'isCrowd': 0, 'area': 18178.0, 'bbox': [212.34375, 114.36823104693141, 57.1875, 51.638989169675085]}]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 1.9920, -0.0629, -0.6109,  ..., -0.5082, -0.5082, -0.5082],\n",
       "          [ 2.1462, -0.1486, -0.6452,  ..., -0.5596, -0.5596, -0.5596],\n",
       "          [ 2.1633, -0.1657, -0.6281,  ..., -0.3369, -0.3369, -0.3369],\n",
       "          ...,\n",
       "          [ 2.0263, -1.3302, -2.1179,  ..., -2.0323, -1.9980, -2.0494],\n",
       "          [ 2.0434, -1.1760, -1.9638,  ..., -2.0665, -2.0837, -2.1179],\n",
       "          [ 2.0092, -1.1589, -1.9124,  ..., -2.1179, -2.0665, -1.8953]],\n",
       " \n",
       "         [[ 2.2710, -0.1099, -0.8803,  ..., -1.0203, -1.0203, -1.0203],\n",
       "          [ 2.2710, -0.5126, -1.2479,  ..., -1.5980, -1.5980, -1.5980],\n",
       "          [ 2.2360, -0.5476, -1.3179,  ..., -1.6155, -1.6155, -1.6155],\n",
       "          ...,\n",
       "          [ 2.3410, -0.9853, -1.7381,  ..., -1.1429, -1.1078, -1.1604],\n",
       "          [ 2.3410, -0.8452, -1.4405,  ..., -1.2129, -1.2129, -1.3179],\n",
       "          [ 2.3235, -0.8102, -1.4230,  ..., -1.4055, -1.2479, -1.0728]],\n",
       " \n",
       "         [[ 2.5006,  0.0779, -0.7413,  ..., -1.0376, -1.0376, -1.0376],\n",
       "          [ 2.5180, -0.2707, -1.0550,  ..., -1.6476, -1.6302, -1.6302],\n",
       "          [ 2.5180, -0.3230, -1.1073,  ..., -1.6999, -1.6999, -1.6999],\n",
       "          ...,\n",
       "          [ 2.5529, -0.7761, -1.5430,  ..., -1.0376, -1.0201, -1.0550],\n",
       "          [ 2.5529, -0.6193, -1.2641,  ..., -1.1247, -1.1247, -1.2119],\n",
       "          [ 2.5529, -0.6018, -1.2293,  ..., -1.2990, -1.1421, -0.9678]]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "\n",
    "example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandboxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
